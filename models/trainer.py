import wandb
from torch.autograd import Variable
import itertools

from .utils import BasicTrainer, median_filter, get_scheduler, print_network
import torch, numpy as np
from .dual_unet import efficient_unet
from .swin import get_swinnet
import os

from . import segloss as segloss

from dataset.utils.transform_utils import get_contrast_example
import torch.nn.functional as F

# interpolating random control points, implemented by Chen Chen et al.
from .aug.biasfield_interpolate_cchen.adv_bias import AdvBias, rescale_intensity
from .aug.imagefilter import GINGroupConv

from .adversarial.network import define_D
from .adversarial.ganloss import GANLoss


class Trainer(BasicTrainer):
    def set_networks(self, opt):
        self.opt = opt
        self.n_cls = opt.nclass
        self.gpu_ids = opt.gpu_ids

        if opt.model == 'efficient_b2_unet':
            self.net = efficient_unet(backbone = opt.backbone, nclass = self.n_cls,
                                      in_channel = opt.in_channel, #gpu_ids = opt.gpu_ids,
                                      sam=opt.sam)

        elif opt.model == 'swinir':
            self.net = get_swinnet(backbone=opt.backbone, nclass=self.n_cls,
                                      in_channel=opt.in_channel, gpu_ids=opt.gpu_ids)

        else:
            raise NotImplementedError

        self.netD = define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D,
                             opt.norm, opt.init_type, opt.init_gain, opt.gpu_ids)


        if opt.resume:
            state_dict = torch.load(opt.resume, map_location='cpu')
            if "model" in state_dict:
                state_dict = state_dict['model']
            self.net.load_state_dict(state_dict, strict=True)
            print("===loading model from: ", opt.resume)
            
        self.net = self.net.cuda(opt.gpu_ids[0])

        self.criterionGAN = GANLoss(opt.gan_mode).cuda()

        # data augmentation nodes
        self.img_transform_node = GINGroupConv(out_channel=opt.gin_out_nc, n_layer=opt.gin_nlayer,
                                               interm_channel=opt.gin_n_interm_ch, out_norm=opt.gin_norm).cuda()
        blender_cofig = {
            'epsilon': opt.blend_epsilon,
            'xi': 1e-6,
            'control_point_spacing': [opt.blend_grid_size, opt.blend_grid_size],
            'downscale': 2,  #
            'data_size': [opt.batchSize, 1, opt.fineSize, opt.fineSize],
            'interpolation_order': 2,
            'init_mode': 'gaussian',
            'space': 'log'
        }
        self.blender_node = AdvBias(blender_cofig)
        self.blender_node.init_parameters()

                    
    def backward_D(self, iter):
        """Calculate GAN loss for the discriminator

        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator

        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """
        
        # self.input_img.detach()
        outputs         = self.net(self.input_img.detach(), dual=True, D=True)
        # pred_all      = outputs['masks']
        target_a        = self.input_img[:self._nb].mean(dim=1, keepdim=True)
        target_a_prime  = self.input_img[self._nb:2*self._nb].mean(dim=1, keepdim=True)
        target_at       = self.input_img[2*self._nb:3*self._nb].mean(dim=1, keepdim=True)
        target_at_prime = self.input_img[3*self._nb:].mean(dim=1, keepdim=True)
        
        
        recons_a        = outputs['a'].mean(dim=1, keepdim=True)
        # recons_a_fake       = outputs['a_mix'].mean(dim=1, keepdim=True)
        recons_at_prime = outputs['a_prime_mix'].mean(dim=1, keepdim=True)  # combine
                
        # same domain
        recon_real_1 = torch.cat([target_a, target_at], dim=1)           # Same domain, Diff dir
        recon_real_2 = torch.cat([target_a_prime, target_a_prime], dim=1)# Same domain, Same dir
        # random revese 
        recon_real_1 = torch.flip(recon_real_1, [1] * (np.random.rand() < 0.5))
        recon_real_2 = torch.flip(recon_real_2, [1] * (np.random.rand() < 0.5))
        
        # Difference domain, Different geometry compare
        recon_fake_1 = torch.cat([recons_a, target_at], dim=1)
        # Difference domain, Same geometry compcare
        recon_fake_2 = torch.cat([recons_at_prime, target_at_prime], dim=1)
            
        recon_fake_3 = torch.cat([target_a, target_a_prime], dim=1)
        recon_fake_4 = torch.cat([target_a, target_at_prime], dim=1)
        
        # random revese 
        recon_fake_1 = torch.flip(recon_fake_1, [1] * (np.random.rand() < 0.5))
        recon_fake_2 = torch.flip(recon_fake_2, [1] * (np.random.rand() < 0.5))
        recon_fake_3 = torch.flip(recon_fake_3, [1] * (np.random.rand() < 0.5))
        recon_fake_4 = torch.flip(recon_fake_4, [1] * (np.random.rand() < 0.5))
        
        
        self.optimizer_D.zero_grad()
        # Real
        loss_D_real = torch.tensor(0, dtype=torch.float).cuda()
        for real in [recon_real_1, recon_real_2]:
            pred_real = self.netD(real)
            l = self.criterionGAN(pred_real, True)
            loss_D_real += l
        loss_D_real = loss_D_real / 2.0
        
        # Fake
        loss_D_fake = torch.tensor(0, dtype=torch.float).cuda()
        for fake in [recon_fake_1, recon_fake_2, recon_fake_3, recon_fake_4]:
            pred_fake = self.netD(fake)
            loss_D_fake += self.criterionGAN(pred_fake, False)
        loss_D_fake = loss_D_fake / 4.0
        
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        self.optimizer_D.step()
        
        if (iter % self.opt.print_iter_freq == 0):
            print(f"Train Dis Step {iter}, "
                f"loss_D: {loss_D:.3f}")
                
        return loss_D


    def initialize(self, opt):
        ## initialize/load the model.
        BasicTrainer.initialize(self, opt)
        self.set_networks(opt)

        if opt.continue_train or opt.phase == 'test':
            try:
                self.load_network_by_fid(self.net, opt.reload_model_fid)
            except:
                print('Cannot load the entire trainer. Trying to reload the segmenter only')
                if hasattr(self.net, 'load_source_model'):
                    self.net.load_source_model(opt.reload_model_fid)
                else:
                    raise Exception('Cannot reload the model')

        ## define loss functions
        self.criterionDice = segloss.SoftDiceLoss(self.n_cls).cuda(self.gpu_ids[0]) # soft dice loss
        self.ScoreDice = segloss.SoftDiceScore(self.n_cls, ignore_chan0 = True).cuda(self.gpu_ids[0]) # dice score
        self.ScoreDiceEval = segloss.Efficient_DiceScore(self.n_cls, ignore_chan0 = False).cuda(self.gpu_ids[0]) # for evaluation in 3D
        self.one_hot_encoder = segloss.One_Hot(self.opt.nclass).forward
        self.mae = torch.nn.L1Loss()
        self.mse = torch.nn.MSELoss()
        
        # using plain CE + Dice loss, not using WCE
        self.criterionWCE = segloss.My_CE(nclass = self.n_cls,\
                batch_size = self.opt.batchSize, weight = torch.ones(self.n_cls,)).cuda(self.gpu_ids[0])

        # consistency between conditional distributions
        if opt.consist_type == 'kld':
            self.kl_loss = torch.nn.KLDivLoss( reduction = 'mean')
        else:
            raise NotImplementedError

        # initialize optimizers
        if self.opt.optimizer == 'adamw':
            self.optimizer_Segmenter = torch.optim.AdamW(itertools.chain(self.net.parameters()), 
                                                        lr=opt.lr,
                                                         betas=(opt.beta1, 0.999),  # 0.9, 0.999
                                                         weight_decay = opt.adam_weight_decay,
                                                         eps = 1e-5)


            self.optimizer_D = torch.optim.AdamW(itertools.chain(self.netD.parameters()),
                                                         lr=opt.lr,
                                                         betas=(opt.beta1, 0.999),  # 0.9, 0.999
                                                         weight_decay=opt.adam_weight_decay,
                                                         eps=1e-5)
        else: 
            raise NotImplementedError

        # register optimizers
        self.optimizers = []
        self.schedulers = []
        self.optimizers.append(self.optimizer_Segmenter)

        # put optimizers into learning rate schedulers
        for optimizer in self.optimizers:
            self.schedulers.append(get_scheduler(optimizer, opt))

        print('---------- Networks initialized -------------')
        print_network(self.net)

        # register subnets which require gradients
        self.subnets = [ self.net ]

        for subnet in self.subnets:
            assert next(subnet.parameters()).is_cuda == True
        print('-----------------------------------------------')

    # bypass the appearance transforms
    def set_input(self, input):
        input_img = input['img']
        input_mask = input['lb']
        input_sam = input['sam']
        if self.opt.sam:
            input_img = torch.cat([input_img, input_sam], dim=-3)

        if len(self.gpu_ids) > 0:
            if not isinstance(input_img, torch.FloatTensor):
                if input_img.ndims < 4:
                    input_img = input_img[np.newaxis, ...]
                input_img = torch.FloatTensor(input_img, requires_grad = False).float()
            input_img = input_img.cuda(self.gpu_ids[0])

            if not isinstance(input_mask, torch.FloatTensor):
                if input_mask.ndims < 4:
                    input_mask = input_mask[np.newaxis, ...]
                input_mask = torch.FloatTensor(input_mask, requires_grad = False).float()
            input_mask = input_mask.cuda(self.gpu_ids[0])

        self.input_img = Variable(input_img)
        self.input_mask = Variable(input_mask)



    def set_input_contrast(self, input):
        '''
        Applying both GIN and IPA

        '''
        input_img    = input['img']
        input_mask   = input['lb']
        input_sam = input['sam']
        sam_mask    = input['sam_mask']
        input_sam = input_sam * sam_mask

        self._nb = input_img.shape[0]
        if len(self.gpu_ids) > 0:
            input_img = input_img.float().cuda(self.gpu_ids[0])
            input_mask = input_mask.float().cuda(self.gpu_ids[0])
            if self.opt.sam:
                input_sam  = input_sam.float().cuda(self.gpu_ids[0])

        # gin
        # input_buffer0 = 0 - self.img_transform_node(0 - input_img)
        input_buffer0 = self.img_transform_node(input_img)
        input_buffer1 = self.img_transform_node(input_img)
        input_buffer = torch.cat([input_buffer0, input_buffer1], dim=0)


        self.blender_node.init_parameters()
        blend_mask = rescale_intensity(self.blender_node.bias_field).repeat(1, 3, 1, 1)
        b, c, h, w = input_buffer.shape

        blend_mask = blend_mask[..., :h, :w]
        # spatially-variable blending
        input_cp1 = input_buffer[: self._nb] * blend_mask + \
                    input_buffer[self._nb: self._nb * 2] * (1.0 - blend_mask)
        input_cp2 = input_buffer[: self._nb] * (1 - blend_mask) + \
                        input_buffer[self._nb: self._nb * 2] * blend_mask

        input_buffer[: self._nb] = input_cp1.clone().detach()
        input_buffer[self._nb: self._nb * 2] = input_cp2.clone().detach()
        
        # self._nb = input_buffer.shape[0]
        input_mask = torch.cat([input_mask, input_mask], dim = 0)
        if self.opt.sam:
            input_sam = torch.cat([input_sam, input_sam], dim=0)
            

        random_angle = np.random.randint(-30, 30)  # 0~360
        flip = np.random.choice([1, 2, 3], replace=False, size=1).tolist()

        img_minmax = input_buffer.clone()         # channel
        img_minmax[img_minmax > 3] = 3
        img_minmax[img_minmax < -3] = -3
        img_minmax = (img_minmax - img_minmax.min()) / (img_minmax.max() - img_minmax.min())
        kernel_size = 25 
        #np.random.choice([13,15,17,19,21,23,25]) #np.random.randint(13, 20)
        img_minmax_blur = median_filter(img_minmax, kernel_size=kernel_size)
        # blur

        self.flip = flip
        self.angle = random_angle
        # 2 * bs
        input_img_r       = get_contrast_example(input_buffer.clone(), random_angle, flip).cuda(self.gpu_ids[0])
        img_minmax_r      = get_contrast_example(img_minmax.clone(), random_angle, flip).cuda(self.gpu_ids[0])
        input_mask_r      = get_contrast_example(input_mask.clone(), random_angle, flip).cuda(self.gpu_ids[0])
        input_sam_r       = get_contrast_example(input_sam.clone(), random_angle, flip).cuda(self.gpu_ids[0])
        img_minmax_blur_r = get_contrast_example(img_minmax_blur.clone(), random_angle, flip).cuda(self.gpu_ids[0])


        # random no-linear augmentation
        # self._nb = input_img.shape[0] # batch size of the current batch

        # img, img_blend_r, img_r, img_blend_r
        input_buffer = torch.cat([input_buffer, input_img_r], dim = 0)
        input_mask_buffer = torch.cat([input_mask, input_mask_r], dim = 0)
        if self.opt.sam:
            input_sam = torch.cat([input_sam, input_sam_r], dim=0)
            input_buffer = torch.cat([input_buffer, input_sam], dim=1)

        # self.input_img_3copy = input_buffer
        self.input_img = input_buffer
        self.input_mask = input_mask
        self.input_mask_contrast = input_mask_buffer
        self.img_minmax = torch.cat([img_minmax, img_minmax_r], dim = 0)
        self.img_blur   = torch.cat([img_minmax_blur, img_minmax_blur_r], dim = 0)
        self.input_img_3copy = input_buffer.clone().detach()


    def get_intermidiate_result_val(self):
        
        print("intermidate_result show:", self.input_img.shape, self.full_val.shape, self.pred_val.shape)
        
        # Horizontal concatenation
        batch_images = [self.input_img, self.full_val, self.anatomical_val, self.domain_val,  self.pred_val, self.gth_val]
        
        
        # self.input_img
        
        # self.full_val       = pred['full']
        # self.anatomical_val = pred['anatomical']
        # self.domain_val     = pred['domain']
    
        # self.pred_val = pred['masks']
        # self.gth_val  = mask_val
            
        pass
    
    # run validation
    def validate(self, save=False):
        for subnet in self.subnets:
            subnet.eval()

        with torch.no_grad():
            img_val = self.input_img
            mask_val = self.input_mask
            
            if save:
                pred  = self.net(img_val, dual=True, D=True)
                self.full_val       = pred['full'].data
                self.anatomical_val = pred['anatomical'].data
                self.domain_val     = pred['domain'].data
                
                
            else:
                pred  = self.net(img_val)

            # now calculating losses!
            loss_dice_val = self.ScoreDice(pred['masks'], mask_val)
            loss_wce_val  = self.criterionWCE(pred['masks'], mask_val.long())

            self.loss_dice_val = loss_dice_val.data
            self.loss_wce_val  = loss_wce_val.data

            # visualizations!
            self.pred_val = pred['masks'].data
            self.gth_val  = mask_val.data

            
            
        for subnet in self.subnets:
            subnet.zero_grad()
            subnet.train()
            
            


    def forward_with_uncertainty(self, batch):
        thr = 0.75
        num_of_pred = 10
        self.set_input(batch)
        preds = torch.zeros([num_of_pred, self.input_img.shape[0], self.n_cls, self.input_img.shape[2], self.input_img.shape[3]]).cuda()
       
        # features = torch.zeros([10, data.shape[0], 305, 128, 128]).cuda()
        for i in range(num_of_pred):
            gth, preds[i,...] = self.get_segmentation_uncertain_gpu(raw_logits = True)
            
        # Step 1.
        pseudo_score     = torch.mean(preds, dim=0).clone()         
        pseudo_raw_label = torch.argmax(torch.softmax(pseudo_score, dim=1), dim=1)
        pseudo_max_score = torch.softmax(pseudo_score, dim=1).max(dim=1, keepdim=True)[0]  # valueï¼Œindex
        pseudo_label     = torch.where(pseudo_max_score > thr, pseudo_raw_label, 0)

        # Step 2. Uncertainty Measurement
        preds = torch.softmax(preds, dim=2)
        std_map = torch.std(preds, dim=0)

        return gth, pseudo_score, pseudo_label, std_map
        

    def get_visualization_gpu(self, raw_logits = False):
        """
        Args:
            raw_logits: output dense masks or logits
        """
        for subnet in self.subnets:
            subnet.eval()

        with torch.no_grad():
            img_val         = self.input_img
            mask_val        = self.input_mask
            outputs         = self.net(img_val, visualization=True)
            full = outputs['full']
            anatomical = outputs['anatomical']
            domain = outputs['domain']
            seg_val = outputs['masks']
            
            full_comp = outputs['full_comp']
            anatomical_comp = outputs['anatomical_comp']
            domain_comp = outputs['domain_comp']
            
            if not raw_logits:
                seg_val = torch.argmax(seg_val, 1)

        for subnet in self.subnets:
            subnet.zero_grad()
            subnet.train()
            
        return mask_val, seg_val, full, anatomical, domain, full_comp, anatomical_comp, domain_comp
    
    
    
    
    def get_segmentation_gpu(self, raw_logits = False):
        """
        Args:
            raw_logits: output dense masks or logits
        """
        for subnet in self.subnets:
            subnet.eval()

        with torch.no_grad():
            img_val         = self.input_img
            mask_val        = self.input_mask
            seg_val         = self.net(img_val)['masks']
            if not raw_logits:
                seg_val = torch.argmax(seg_val, 1)

        for subnet in self.subnets:
            subnet.zero_grad()
            subnet.train()
        return mask_val, seg_val

    def get_segmentation_uncertain_gpu(self, raw_logits = False):
        """
        Args:
            raw_logits: output dense masks or logits
        """

        with torch.no_grad():
            img_val         = self.input_img
            mask_val        = self.input_mask
            seg_val         = self.net(img_val)['masks']
            if not raw_logits:
                seg_val = torch.argmax(seg_val, 1)
            
            for subnet in self.subnets:
                subnet.zero_grad()
                
        return mask_val, seg_val

    def divergence(self, x):
        grad_0 = torch.gradient(x, dim=1)
        grad_1 = torch.gradient(x, dim=2)
        grad_2 = torch.gradient(x, dim=3)

        # torch.autograd.gradient

        return grad_0 + grad_1 + grad_2


    def forward_seg_train_contrast(self, input_img):
        """
        run a forward segmentation in training mode
        """
        lambda_Seg   = self.opt.lambda_Seg 
        lambda_wce   = self.opt.lambda_wce 
        lambda_dice  = self.opt.lambda_dice

        # a, a', at, at'
        target_a        = input_img[:self._nb]          # Only compute the mask for this
        # target_a_prime  = input_img[self._nb:2*self._nb]
        # target_at       = input_img[2*self._nb:3*self._nb]
        target_at_prime = input_img[3*self._nb:]
        
        outputs      = self.net(input_img, dual=True, D=True, partial=self._nb)
        pred_all     = outputs['masks']

        recons_a            = outputs['a']
        recons_at_prime     = outputs['a_prime_mix']
        
        domain_feature_a  = outputs['domain_a']
        domain_feature_at = outputs['domain_at'] 
        
        # Too Large
        if self.opt.use_discriminator:
            recon_true_1 = torch.cat([recons_a, target_a], dim=1) 
            recon_true_2 = torch.cat([recons_at_prime, target_at_prime], dim=1)
        
            domain_loss = self.criterionGAN(recon_true_1, True) + \
                        self.criterionGAN(recon_true_2, True)     # * 10, make it 
            domain_loss = domain_loss * 0.05
            
        else:
            domain_loss = 0   # domain_loss * 0.1
        
        # 1 Channel
        loss_recon = self.mse(recons_a[:, 0],        target_a[:, target_a.shape[1]//2]) + \
                     self.mse(recons_at_prime[:, 0], target_at_prime[:, target_a.shape[1]//2])        # * 0.1 # 0.942 loss  
        
        
        preds        = pred_all          # B, C, H, W       
        masks        = self.input_mask_contrast[:3*self._nb]    #[:self._nb]
        loss_dice    = self.criterionDice(input = preds, target = masks)
        loss_wce     = self.criterionWCE(inputs = preds, targets = masks.long() )

        # Add segment - recon loss
        # loss_seg_recon = 0
        # batch_size = preds.size(0)
        # h, w = preds.size(2), preds.size(3)

        # preds_sm = F.softmax(preds, dim=1).view(batch_size, self.n_cls, h, w)
        # masks_onehot = self.one_hot_encoder(masks).contiguous().view(batch_size, self.n_cls, h, w)
    
        # for idx in range(self.n_cls):
        #     l = (preds_sm[:self._nb, idx]     * recons_a[:self._nb, 0].clone().detach() - 
        #          masks_onehot[:self._nb, idx] * target_a[:self._nb, target_a.shape[1]//2])
        #     l = l * l
        #     l = l.sum() / torch.sum(masks_onehot[:, idx] + 1e-5)
                
        #     if loss_seg_recon == 0:
        #         loss_seg_recon = l
        #     else:      
        #         loss_seg_recon += l
        
        # loss_recon = loss_recon + loss_seg_recon.mean() / self.n_cls
        
        feature_loss = self.get_feature_loss(domain_feature_a, domain_feature_at, kl=True) * 0.5  #, 0

        # BCHW

        self.seg_tr         = preds.clone().detach()
        self.loss_seg       = (loss_dice   *  lambda_dice + 
                               loss_wce    *  lambda_wce +    
                               loss_recon   + domain_loss + feature_loss) * lambda_Seg  #

        self.domain_loss    = domain_loss
        self.loss_seg_tr    = self.loss_seg.data
        self.loss_dice      = (loss_dice).data
        self.loss_wce       = (loss_wce).data
        self.loss_recon     = loss_recon #.data
        self.loss_feature   = feature_loss # .data

        return preds, pred_all, 0

    def get_feature_loss(self, feat_a, feat_b, kl=False):
        feat_loss = []
        feat_a = feat_a[1:]
        feat_b = feat_b[1:]
        
        # BCHW
        for idx, (i, j) in enumerate(zip(feat_a, feat_b)):

            # the largest element in the vector will become it rules out the possibility of overflow
            # feat_all = feat_all - torch.max(feat_all, dim=1, keepdim=True).values
            i = i - i.max(dim=1, keepdim=True).values
            j = j - j.max(dim=1, keepdim=True).values

            # feat_all
            feat_all = torch.cat([i, j], axis=0)
            feat_prob = F.softmax(feat_all, dim=1)

            feat_avg = 1.0 / 2.0 * (feat_prob[:i.shape[0]] + feat_prob[i.shape[0]:])
            feat_avg = torch.cat([feat_avg for ii in range(2)], dim=0)

            if kl:
                feat_all = F.log_softmax(feat_all, dim=1)
                loss     = self.kl_loss( feat_all, feat_avg.clone().detach() )  # used dont detach

            else:
                feat_all = F.softmax(feat_all, dim=1)
                loss     = self.mae( feat_all, feat_avg.clone().detach() ) # used dont detach

            feat_loss.append(loss.mean())

        output = torch.stack(feat_loss).sum()
        return torch.nan_to_num(output, nan=1.0, posinf=1.0, neginf=1.0) # * 10
    
    
    
    def get_atonomy_loss(self, diagnosis_feat, kl=False):
        feat_loss = []

        # BCHW
        for i, feat in enumerate(diagnosis_feat[1:]):
            feat_a = feat[:2*self._nb]   # 2B
            feat_b = feat[2*self._nb:]   # 1B
            feat_a = get_contrast_example(feat_a, self.angle, self.flip)
            feat_all = torch.cat([feat_a, feat_b], axis=0)  # Aligned

            # the largest element in the vector will become it rules out the possibility of overflow
            # feat_all = feat_all - torch.max(feat_all, dim=1, keepdim=True).values
            feat_all = feat_all - feat_all.max(dim=1, keepdim=True).values

            # feat_all
            feat_prob = F.softmax(feat_all, dim=1)

            feat_avg = 1.0 / 3.0 * (feat_prob[:self._nb] +
                                    feat_prob[self._nb:2*self._nb] +
                                    feat_prob[2*self._nb:])
            feat_avg = torch.cat([feat_avg for ii in range(3)], dim=0)

            if kl:
                feat_all = F.log_softmax(feat_all, dim=1)
                loss     = self.kl_loss( feat_all, feat_avg.clone().detach() )  # used dont detach

            else:
                feat_all = F.softmax(feat_all, dim=1)
                loss     = self.mae( feat_all, feat_avg.clone().detach() ) # used dont detach

            feat_loss.append(loss.mean())

        output = torch.stack(feat_loss).sum()
        return torch.nan_to_num(output, nan=1.0, posinf=1.0, neginf=1.0) # * 10

    def get_domain_loss(self, domain_feat, kl=False):
        feat_loss = []

        # BCHW
        for i, feat in enumerate(domain_feat[1:]):
            # BCHW
            B, C, H, W = feat.shape
            feat_a = feat[0:self._nb]#.view(self._nb, C, -1)  # flatten
            feat_b = feat[2*self._nb:]#.view(self._nb, C, -1)

            feat_all = torch.cat([feat_a, feat_b], axis=0)

            feat_all_a = feat_all.clone()
            feat_all_b = feat_all.clone()

            # Vertically
            feat_all_a = feat_all_a - torch.max(feat_all_a, dim=2, keepdim=True).values
            feat_prob_a = F.softmax(feat_all_a, dim=2)
            feat_avg_a = 1.0 / 2.0 * (feat_prob_a[:self._nb] + feat_prob_a[self._nb:2*self._nb])
            feat_avg_a = torch.cat([feat_avg_a for ii in range(2)], dim=0)

            # Horizontally
            feat_all_b = feat_all_b - torch.max(feat_all_b, dim=3, keepdim=True).values
            feat_prob_b = F.softmax(feat_all_b, dim=3)
            feat_avg_b = 1.0 / 2.0 * (feat_prob_b[:self._nb] + feat_prob_b[self._nb:2 * self._nb])
            feat_avg_b = torch.cat([feat_avg_b for ii in range(2)], dim=0)

            if kl:
                feat_all_a = F.log_softmax(feat_all_a, dim=-1)
                loss_a = self.kl_loss( feat_all_a, feat_avg_a.clone().detach() )

                feat_all_b = F.log_softmax(feat_all_b, dim=-1)
                loss_b = self.kl_loss(feat_all_b, feat_avg_b.clone().detach())
                loss = loss_a + loss_b

            else:
                feat_all_a = F.softmax(feat_all_a, dim=-1)
                loss_a = self.mae(feat_all_a, feat_avg_a.clone().detach())

                feat_all_b = F.softmax(feat_all_b, dim=-1)
                loss_b = self.mae(feat_all_b, feat_avg_b.clone().detach())
                loss = loss_a + loss_b

            # if not torch.isnan(loss):
            feat_loss.append(loss.mean())

        # if feat_loss == []:
            # return 1.0
        output = torch.stack(feat_loss).sum()
        return torch.nan_to_num(output, nan=1.0, posinf=1.0, neginf=1.0) # * 10


    def forward_contrast_consistency(self, pred_all, recon_all):
        '''
        KL-term, enforcing conditional distribution remains unchanged regardless of interventions applied
        '''

        lambda_consist = self.opt.lambda_consist

        pred_a = pred_all[:2*self._nb]
        pred_b = pred_all[2*self._nb:]

        pred_a = get_contrast_example(pred_a, self.angle, self.flip)

        pred_all = torch.cat([pred_a, pred_b]) # 4B
        pred_all = pred_all - torch.max(pred_all, dim=1, keepdim=True).values
        
        pred_all_prob = F.softmax(pred_all, dim=1) # Distribution
        
        pred_avg = 1.0 / 3 * (pred_all_prob[:self._nb] +  
                              pred_all_prob[self._nb  :2*self._nb] +
                              pred_all_prob[2*self._nb:3*self._nb]) # 4B 
        
        # efficient implementation inspired by Xu et al. (Randconv)
        pred_avg = torch.cat([pred_avg for ii in range(3)], dim=0)
        
        pred_all = F.log_softmax(pred_all, dim=1)
        self.pred_consist = self.kl_loss( pred_all, pred_avg.clone().detach() )  # 0 ~ 1

        #self.kl_loss( log_sigmoid(recon_combine), torch.sigmoid(recon_avg))
        loss_consist = ( self.pred_consist )  #  + self.recon_consist
        loss_consist = torch.nan_to_num(loss_consist, nan=1.0, posinf=1.0, neginf=1.0)

        self.loss_consist_contrast = lambda_consist * loss_consist
        self.loss_consist_tr = self.loss_consist_contrast.data


    def optimize_parameters_contrast(self, iter, opt):
        self.set_requires_grad(self.subnets, True)
        pred, pred_all, recon_all = self.forward_seg_train_contrast(self.input_img.detach())

        # calculate loss
        self.forward_contrast_consistency(pred_all, recon_all)
        
        self.optimizer_Segmenter.zero_grad()
        loss = self.loss_seg + self.loss_consist_contrast
        ( loss ).backward()  #
        
        grad_norm = torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm = 100.0)
        
        if torch.isnan(self.loss_seg):
            print("contrast optimizer = NAN")
            print(f"=== pred: {pred.max()},  {pred.min()}, {pred.shape},"
                  f"input: {self.input_img.max()},  {self.input_img.min()}, {self.input_img.shape},"
                  f"mask: {self.input_mask_contrast.max()}, {self.input_mask_contrast.min()}, {self.input_mask_contrast.shape}")
            return
        
                
        self.optimizer_Segmenter.step()
        self.set_requires_grad(self.subnets, False)

        wandb.log({"con_loss_dice": self.loss_dice,
                   f"con_loss_wce": self.loss_wce,
                   f"con_loss_recon": self.loss_recon,
                   f"con_loss_feature": self.loss_feature,
                   f"loss_consist": self.loss_consist_contrast.data})
                 

        if (iter % opt.print_iter_freq == 0):
            print(f"Contrast Step {iter}, "
                  f"loss_dice: {self.loss_dice:.3f}, "
                  f"loss_wce: {self.loss_wce:.3f}, "
                  f"loss_recon: {self.loss_recon:.3f}, "
                  f"loss_domain: {self.domain_loss:.3f}, "
                  f"loss_feature: {self.loss_feature:.3f}, "
                  f"loss_consist: {self.loss_consist_contrast.data:.3f}")

        return not torch.isnan(loss)

    # NOTE: remeber to modify this when expanding the model if more than one network component need to be stored
    def save(self, label):
        self.save_network(self.net, 'Seg', label, self.gpu_ids)

    def load(self, epoch_label, network_label="Seg"):
        # self.save_network(self.net, 'Seg', label, self.gpu_ids)
        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)
        save_path = os.path.join(self.snapshot_dir, save_filename)

        state_dict = torch.load(save_path, map_location='cpu')
        if "model" in state_dict:
            state_dict = state_dict['model']
        self.net.load_state_dict(state_dict, strict=True)
        self.net.cuda()
        print("===resume model from: ", save_path)


    def set_requires_grad(self, nets, requires_grad=False):
        """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
        Parameters:
            nets (network list)   -- a list of networks
            requires_grad (bool)  -- whether the networks require gradients or not
        """
        if not isinstance(nets, list):
            nets = [nets]
        for net in nets:
            if net is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad

